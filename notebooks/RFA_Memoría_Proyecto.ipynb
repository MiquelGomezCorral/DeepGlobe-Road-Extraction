{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# RFA Trabjo Pr√°cticas: DeepGlobe-Road-Extraction con DataAgumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Descripci√≥n del proyecto\n",
    "Este proyecto pretente abordar el problema de la `road extraction` / `road segmentataion`. Es decir, a partir de im√°genes satelit√°les, extrar en que lugares hay carrertas y qu√© lugares no. O, mejor dicho: predecir para cualquier imagen satelital, qu√© lugares tienen carretera (`1`) y qu√© lugares son solo 'fondo' (`0`). La tarea se podr√≠a comparar (a menor escala) a lo que tuvo que haber hecho Google al hacer los mapas de carreras en Google Maps.\n",
    "\n",
    "El proyecto se ha enfocado con la idea de, a partir de un dataset, y tras generar las particiones de datos correspondientes: entrenar diferentes modelos con distintas configuraciones, con la idea de encontrar cu√°l es la que mejores resultados obtiene en la tarea. Tras entrenar todos los modelos, se pondr√°n a prueba con un conjunto de datos reservado para test, del que se podr√°n sacar conclusiones.\n",
    "\n",
    "En concreto, se probar√°n diferentes arquitecturas, diferentes funciones de p√©rdida y sobre todo, diferentes aumentos de datos.\n",
    "\n",
    "# Dataset\n",
    "El dataset base utilizado es el `DeepGlobe Road Extraction`. Se ha particionado de este dataset, se ha escogido la partici√≥n de entranamiento √∫nicamente por ser la √∫nica totalmente etiquetada. A partir de ella se han creado otros subconjunto de entrenamiento / validaci√≥n / test en un 70%, 15%, 15% respectivamente. \n",
    "\n",
    "Se ha elegido este dataset por estar bastate bien documentado, con suficientes referencias y con un tama√±o de datos elevado.\n",
    "\n",
    "# Data agumentation\n",
    "Esta t√©cnica es uno de los puntos m√°s interesantes del proyecto, ya que permite ampliar la variedad de ejemplos que ve el modelo y simular condiciones reales (*rotaciones, espejado, cambios de color, ruido, recortes, etc.)*, lo que suele mejorar la capacidad de generalizaci√≥n y la robustez de los modelos.\n",
    "\n",
    "### Ventajas\n",
    "- Aumenta la diversidad de datos sin etiquetar m√°s im√°genes reales, reduciendo overfitting y reduciendo el coste que implica conseguir m√°s datos.\n",
    "- Simula situaciones y variaciones (*√°ngulo, iluminaci√≥n, ruido*), √∫til para modelos que deben generalizar a nuevas zonas no vistas.\n",
    "- Algunas transformaciones (*shift de color, ruido*) ayudan a que el modelo no dependa de colores/contrastes espec√≠ficos de una regi√≥n.\n",
    "\n",
    "### Desventajas / riesgos\n",
    "- Si la augmentaci√≥n es demasiado agresiva, puede romper la coherencia entre imagen y m√°scara (*p. ej. recortes que cortan carreteras importantes*) y degradar la calidad del aprendizaje.\n",
    "- Aumentos no realistas (*colores extremos, formas artificiales*) pueden ense√±ar al modelo patrones que no existen en el mundo real y empeorar la generalizaci√≥n.\n",
    "- Augmentaciones que cambian geometr√≠a (*ej. warps fuertes*) pueden alterar la topolog√≠a de la red de carreteras y producir predicciones ‚Äúfantasma‚Äù.\n",
    "- Es d√≠ficil saber si aplicamos mucha o poca augmentaci√≥n.\n",
    "\n",
    "### Mucho versus poco aumento\n",
    "- Poca augmentaci√≥n: riesgo de sobreajuste a las condiciones del dataset (*p. ej. mismas zonas, mismas horas/delimitaciones*). Esto termina en un mejor rendimiento en datos muy parecidos al train, pero peor en nuevos dominios.\n",
    "- Mucha augmentaci√≥n: mejora robustez en escenarios variados, pero puede empeorar convergencia y precisi√≥n si las transformaciones desnaturalizan las se√±ales relevantes (*anchura/contraste de carreteras*). Suele haber una ventana √≥ptima de intensidad/probabilidad de aplicar cada transformaci√≥n.\n",
    "\n",
    "### Adversarial learning y augmentaci√≥n `?????????????????????'`\n",
    "Una parte importante del estudio en el aprendizaje automatico es el uso de `estrategias adversariales`. Estas conllevan entrenar un discriminador que diferencie segmentaciones reales de generadas, o utilizar perturbaciones adversariales durante entrenamiento. Esta t√©ncia ha mostrado mejorar robustez y, en algunos trabajos, la conectividad y calidad de las m√°scaras en tareas de segmentaci√≥n sem√°ntica y extracci√≥n de v√≠as. Ejemplos y revisiones muestran enfoques semi-supervisados y basados en GANs aplicados a segmentaci√≥n y road extraction. \n",
    "faculty.ucmerced.edu\n",
    "\n",
    "### Transformaciones aplicada (y por qu√© tienen sentido)\n",
    "- **ROTATE**: rotaciones aleatorias: ayuda a invariancia rotacional. √ötil si las carreteras aparecen en cualquier orientaci√≥n.\n",
    "- **MIRROR**: espejado horizontal/vertical. Aumenta variaci√≥n geom√©trica sin cambiar etiquetas.\n",
    "- **SUB**: sub-crop (recorte). Muestra patches locales y obliga al modelo a decidir con contexto reducido. *Hay que tener cuidado ya que las im√°genes pueden perder la continuidad*.\n",
    "- **CIRCLES**: a√±adir c√≠rculos con color igual a la media de la imagen. Se usa la estrategia de elegir parches con un color 'homogeneo' para evitar introducir artefactos negros que el modelo asocie err√≥neamente con carreteras (p*or ejemplo sombras grandes*). Usar el color medio reduce el riesgo de introducir un sesgo crom√°tico artificial y ayuda a que el modelo no aprenda ‚Äúnegro  = carretera‚Äù. *Decimos negro pero con cualquier color fijo se aplica la misma l√≥gica*.\n",
    "- **SHIFT_COLOR**: variaci√≥n del tono. Simula diferentes condiciones de iluminaci√≥n/sensores.\n",
    "- **NOISE**: a√±adir ruido. Mejora robustez frente a compresi√≥n de im√°genes y sensores ruidosos.\n",
    "\n",
    "Se aplica cada transformaci√≥n con una probabilidad razonable `(p.ej. 0.2‚Äì0.5)` y, en general, controla la fuerza de la transformaci√≥n por validaci√≥n. Registrar qu√© combinaciones/probabilidades funcionan mejor es tan importante como el propio experimento.\n",
    "\n",
    "# Modelos\n",
    "Se pretende utilizar cuatro arquitecturas diferentes, las cuales podemos separar en dos tipos: Redes neuronales Convolucionales y Vision transformers. Del primer tipo solo tenemos un modelo, pero ser√° suficiente para compara la versatilidad de estos en tareas de visi√≥n.\n",
    "\n",
    "En concreto, se usar√°n estas arquit√©ctuas:\n",
    "\n",
    "- **U-Net**: Arquitectura en forma de U con camino de contracci√≥n (encoder) y expansi√≥n (decoder) y skip connections que recuperan detalles espaciales finos. Fue dise√±ada para segmentaci√≥n con pocos datos y enfatiza el uso de augmentaci√≥n y entrenamiento end-to-end para obtener buena localizaci√≥n. (Ronneberger et al., 2015). \n",
    "arXiv\n",
    "\n",
    "- **FPN (Feature Pyramid Network)**: Construye una pir√°mide de caracter√≠sticas multi-escala mediante un camino top-down con conexiones laterales; permite explotar jerarqu√≠as de la CNN para detectar/segmentar objetos de distintos tama√±os con coste marginal. Muy √∫til para captar carreteras de diferentes anchuras. (Lin et al., CVPR 2017). \n",
    "CVF Open Access\n",
    "\n",
    "- **PSPNet (Pyramid Scene Parsing Network)**: Introduce un m√≥dulo de pyramid pooling que agrega informaci√≥n de contexto global a distintas escalas (regiones grandes‚Üípeque√±as), mejorando la coherencia sem√°ntica y la predicci√≥n en escenas complejas. Bueno para incorporar contexto amplio (p. ej. distinguir carretera de parche sombreado). (Zhao et al., CVPR 2017). \n",
    "CVF Open Access\n",
    "\n",
    "- **ViT / Transformers para segmentaci√≥n**:  Aplican self-attention a parches de imagen para modelar dependencias globales desde las primeras capas; pueden usarse como encoder puro (SETR) o h√≠brido (encoder transformer + decoder convolucional). Mejora el contexto global y las relaciones a larga distancia, √∫til cuando la estructura de la red de carreteras depende de contexto amplio. (SETR ‚Äî Zheng et al., 2021). \n",
    "arXiv\n",
    "\n",
    "\n",
    "### Funciones de p√©rdida \n",
    "En un experimetno as√≠, probar varias funciones de p√©rdida es √∫til porque la p√©rdida `gu√≠a el aprendizaje`: cambia qu√© errores el modelo prioriza, c√≥mo se comportan los gradientes y, en la pr√°ctica, afecta a la precisi√≥n, la robustez y la conectividad de las carreteras predichas. Las m√©tricas que usadas en el proyecto han sido:\n",
    "\n",
    "- **DiceLoss**: Basada en la m√©trica Dice / F1; com√∫n y robusta frente al desequilibrio de clases porque maximiza la superposici√≥n entre predicci√≥n y etiqueta. Muy usada en segmentaci√≥n m√©dica y en muchos trabajos de segmentaci√≥n de carreteras (uso cl√°sico: V-Net / Dice). \n",
    "arXiv\n",
    "\n",
    "- **BCEWithLogitsLoss**: Binary Cross Entropy aplicada sobre logits (incluye la sigmoid internamente, m√°s num√©ricamente estable). Es adecuada para problemas binarios pero puede sufrir cuando las clases est√°n muy desbalanceadas, ya que penaliza por p√≠xel y la mayor√≠a son fondos. (PyTorch docs explican el uso y estabilidad). \n",
    "docs.pytorch.org\n",
    "\n",
    "- **BCE + Dice (composite loss)**: Combinaci√≥n pr√°ctica y extendida: la BCE aporta gradientes m√°s ‚Äúsuaves‚Äù y la Dice compensa el desequilibrio maximizando la m√©trica objetivo. Por eso se suele entrenar con una p√©rdida compuesta Loss = BCEWithLogits + Œ±¬∑DiceLoss (Œ± en [0.5,1] t√≠pico). La justificaci√≥n pr√°ctica: la BCE sola no porque tiene problemas con balanceo de clases y pues la mayor√≠a NO es road, es background entonces...\n",
    "\n",
    "- **IoU (Intersectoin over Union)**: La IoU calcula la superficie que ha sido bien predicha, entre la cantidad total de superficia que ha sido predicha entre la referencia y el modelo. Esto crea una proporci√≥n de 'entre todo lo que hab√≠a, cuanto ha sido bien predicho'. Est√° funci√≥n de p√©rdida no se usa para entrenamiento sino p√°s bien para hacer el testing. Algunas referencias la marcan como una funci√≥n que puede manejar mejor el desequilibrio, pero la combinaci√≥n BCE+Dice es un buen punto de partida y frecuente en la literatura para el entrenamiento. Es por ello que NO la usaremos para entrenar pero s√≠ para hacer testing. Una vez todos los modelos est√©n entrenado, ser√° nuestra m√©trica. \n",
    "\n",
    "\n",
    "\n",
    "# Trabajos de referencia `¬ø¬ø¬ø¬ø¬ø¬ø¬ø¬ø¬ø¬ø¬ø¬ø¬ø¬ø¬°¬°¬°`\n",
    "Hung, W.-C. et al., Adversarial Learning for Semi-Supervised Semantic Segmentation (BMVC 2018) ‚Äî adversarial training aplicado a segmentaci√≥n, √∫til para semi-supervisi√≥n y robustez. \n",
    "faculty.ucmerced.edu\n",
    "\n",
    "Chen, H. et al., SW-GAN / Semi-weakly supervised methods for road extraction (MDPI) ‚Äî ejemplos de GANs y aprendizaje adversarial aplicados a extracci√≥n de carreteras. \n",
    "MDPI\n",
    "\n",
    "Milletari, V. et al., V-Net (2016) ‚Äî introducci√≥n / uso del Dice loss en segmentaci√≥n (referencia cl√°sica). \n",
    "arXiv\n",
    "\n",
    "PyTorch ‚Äî documentaci√≥n de BCEWithLogitsLoss para referencia t√©cnica sobre la implementaci√≥n estable num√©ricamente. \n",
    "docs.pytorch.org\n",
    "\n",
    "Liu, R. et al., A Review of Deep Learning-Based Methods for Road Extraction (Remote Sensing, 2024) ‚Äî revisi√≥n reciente que te sirve para contextualizar t√©cnicas actuales y comparativas. \n",
    "MDPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Implementaci√≥n del c√≥digo\n",
    "\n",
    "Todo el c√≥digo se encuentra en este repositorio, m√°s en concreto de forma accesible en [app](../app/main.py). En el [README.md](../README.md) se explica con m√°s detalles como usar el repositorio y todo su fucionamiento.\n",
    "\n",
    "En resumidas cuentas, se usa [main.py](../app/main.py) como punto de entrada a la aplicaci√≥n para llamar a todos los scripts, y desde ah√≠ se orquesta toda la funcionalidad del repositorio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# An√°lisis de los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Configuraci√≥n del notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "if not os.getcwd().endswith(\"app\"):\n",
    "    os.chdir(\"../app\")\n",
    "    print(os.getcwd())\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Carga de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config import Configuration\n",
    "from maikol_utils.file_utils import list_dir_files\n",
    "from src.utils import PathParser\n",
    "\n",
    "CONFIG = Configuration()\n",
    "\n",
    "files, n = list_dir_files(CONFIG.LOGS_FOLDER, recursive=True)\n",
    "models = [f for f in files if \"checkpoints\" in f]\n",
    "metrics = [f for f in files if \"test_metrics\" in f]\n",
    "    \n",
    "model_obj = [PathParser(m) for m in models]\n",
    "metrics_obj = [PathParser(m) for m in metrics]\n",
    "model_metrics = [(mod, met) for mod in model_obj for met in metrics_obj if mod.name == met.name]\n",
    "\n",
    "len(model_obj)\n",
    "len(metrics_obj)\n",
    "len(model_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Tiempo de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.script_refactor import get_minutes\n",
    "\n",
    "model_hours = [(mod, get_minutes(met.path)) for mod, met in model_metrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Separate groups\n",
    "vit_times = [m for mod, m in model_hours if \"ViT\" in mod.name]\n",
    "cnn_times = [m for mod, m in model_hours if \"ViT\" not in mod.name]\n",
    "all_times = [m for _, m in model_hours]\n",
    "\n",
    "# Boxplot\n",
    "plt.boxplot([all_times, vit_times, cnn_times], labels=[\"All\", \"ViT\", \"CNN\"])\n",
    "plt.ylabel(\"Training time (minutes)\")\n",
    "plt.title(\"Training time distribution per model type\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "<span style=\"color:LightGreen\"> \n",
    "Vemos que no hay diferencia significativa entre los tiempos de entrenamietno\n",
    "Hay alg√∫n que otro entrenamiento que toma mucho m√°s tiempo, pero no de forma general\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Resultados de infernecia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maikol_utils.file_utils import load_json\n",
    "from src.utils import plot_model_scores\n",
    "\n",
    "cnn_models_scores = load_json(CONFIG.cnn_model_scores)\n",
    "vit_models_scores = load_json(CONFIG.vit_model_scores)\n",
    "\n",
    "all_models_scores = {**cnn_models_scores, **vit_models_scores}\n",
    "\n",
    "plot_model_scores(all_models_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import plot_model_scores_by_architecture\n",
    "\n",
    "# Plot models grouped by architecture with data augmentation coloring\n",
    "plot_model_scores_by_architecture(all_models_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### An√°lisis con Box Plots\n",
    "\n",
    "Los box plots nos permiten ver la distribuci√≥n de IoU para diferentes grupos, mostrando mediana, cuartiles, y valores at√≠picos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar la nueva funci√≥n de box plots\n",
    "from src.utils.visualizations import plot_iou_boxplots_by_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot agrupado por ARQUITECTURA\n",
    "print(\"üìä Box Plot: IoU por Arquitectura\")\n",
    "print(\"=\" * 50)\n",
    "plot_iou_boxplots_by_parameter(all_models_scores, group_by='ARC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot agrupado por FUNCI√ìN DE P√âRDIDA\n",
    "print(\"üìä Box Plot: IoU por Funci√≥n de P√©rdida\")\n",
    "print(\"=\" * 50)\n",
    "plot_iou_boxplots_by_parameter(all_models_scores, group_by='LSS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot agrupado por DATA AUGMENTATION\n",
    "print(\"üìä Box Plot: IoU por Data Augmentation\")\n",
    "print(\"=\" * 50)\n",
    "plot_iou_boxplots_by_parameter(all_models_scores, group_by='AUG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# An√°lisis del data augmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
